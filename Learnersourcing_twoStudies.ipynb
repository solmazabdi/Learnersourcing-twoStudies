{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/analytics/shared/packages/')\n",
    "import os\n",
    "import csv\n",
    "import traceback\n",
    "def prvar(__x):\n",
    "    print traceback.extract_stack(limit=2)[0][3][6:][:-1],\"=\",__x\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from math import sqrt\n",
    "import random\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from subprocess import check_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alias names for CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_folder = 'data_files'\n",
    "DE_ratings_file = 'expert_rating.csv' #CSV file containing domain expert ratings\n",
    "S_rating_file = 'student_ratings.csv' #CSV file containing student ratings\n",
    "S_performance_file = 'student_performance.csv' #CSV file containing student performance level obtained from their scores in the course\n",
    "q_difficulty_file = 'question_difficulty.csv' # CSV file containing question difficulty and question ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(Input_folder, data_file):\n",
    "    '''\n",
    "    load data sets into pandas data frames\n",
    "    '''\n",
    "    filePath = Input_folder + '/' + data_file\n",
    "    df = pd.read_csv(filePath, encoding='UTF-8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def by_performance_ratings(performane_df, quality_rating_df):\n",
    "    '''\n",
    "    merging high_perfroming student_performance and \n",
    "    quality ratings dataframes to have ratings based on students performance\n",
    "    \n",
    "    returns 3 dataframes containing ratings given by each group of students.\n",
    "    '''\n",
    "    by_performane_rating_df = pd.merge(quality_rating_df, performane_df, on=['UserID'])\n",
    "    ratings_and_high_pf = by_performane_rating_df[by_performane_rating_df.Performance_group == 'High-performing']\n",
    "    ratings_and_average_pf = by_performane_rating_df[by_performane_rating_df.Performance_group == 'Average-performing']\n",
    "    ratings_and_low_pf = by_performane_rating_df[by_performane_rating_df.Performance_group == 'Low-performing']\n",
    "    return ratings_and_high_pf, ratings_and_average_pf, ratings_and_low_pf \n",
    "\n",
    "\n",
    "def by_performance_rating_aggregation(by_performane_rating_df, group_mean, group_std):    \n",
    "    '''\n",
    "    Applies mean aggregation on the ratings given by each group of students\n",
    "    '''\n",
    "    by_performance_effectiveness_stats = by_performane_rating_df.groupby(['QID'])['effectiveness'].agg([np.mean, np.std])\n",
    "    by_performance_effectiveness_stats = by_performance_effectiveness_stats.reset_index()\n",
    "    by_performance_effectiveness_stats = by_performance_effectiveness_stats.rename(columns={'mean': group_mean, 'std': group_std})\n",
    "    return by_performance_effectiveness_stats\n",
    "\n",
    "\n",
    "def merge_all_student_ratings(effectiveness_rating_and_high_performing, effectiveness_rating_and_average_performing, effectiveness_rating_and_low_performing, quality_ratings_df, Study_Answers_diff):\n",
    "    '''\n",
    "    Calls the aggregation function on the ratings given by each group of students\n",
    "    and then merges the aggregation results to form a uniform data set\n",
    "    '''\n",
    "    high_performing_effectiveness_stats = by_performance_rating_aggregation(effectiveness_rating_and_high_performing, 'High-mean', 'High-std')\n",
    "    average_performing_effectiveness_stats = by_performance_rating_aggregation(effectiveness_rating_and_average_performing, 'Average-mean', 'Average-std')\n",
    "    low_performing_effectiveness_stats = by_performance_rating_aggregation(effectiveness_rating_and_low_performing, 'Low-mean', 'Low-std')\n",
    "    ratings_eff_class_df = by_performance_rating_aggregation(quality_ratings_df, 'Class-mean', 'Class-std')\n",
    "    \n",
    "    students_effectiveness_ratings = pd.merge(low_performing_effectiveness_stats, high_performing_effectiveness_stats, on=['QID'])\n",
    "    students_effectiveness_ratings = pd.merge(students_effectiveness_ratings, average_performing_effectiveness_stats, on=['QID'])\n",
    "    students_effectiveness_ratings = pd.merge(students_effectiveness_ratings, ratings_eff_class_df, on=['QID'])\n",
    "    students_effectiveness_ratings = pd.merge(students_effectiveness_ratings, Study_Answers_diff, on=['QID'])\n",
    "\n",
    "    return students_effectiveness_ratings\n",
    "\n",
    "\n",
    "def create_complete_dataframe(students_effectiveness_ratings, experts_ratings):\n",
    "    '''\n",
    "    This function merges the dataframe contaning the given \n",
    "    ratings by each group of students to each question, with \n",
    "    the the dataframe containing given ratings by domain experts \n",
    "    to each question\n",
    "    '''\n",
    "    return pd.merge(students_effectiveness_ratings, experts_ratings[['QID', 'Expert Ratings', 'Expert -std']], on=['QID'])\n",
    "\n",
    "\n",
    "def by_bin_ratings(QIDS_by_bin, ratings):\n",
    "    '''\n",
    "    merges ratings given by each group of students to each bin of resources\n",
    "    '''\n",
    "    by_bin_ratings = pd.merge(QIDS_by_bin, ratings, on = ['QID'])\n",
    "    return by_bin_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load student ratings, student performnce and expert ratings into 3 data frames\n",
    "quality_ratings_df = load_data(Input_folder, S_rating_file) \n",
    "performance_df = load_data(Input_folder, S_performance_file)\n",
    "expert_ratings_df = load_data(Input_folder, DE_ratings_file)\n",
    "question_difficulty_df = load_data(Input_folder, q_difficulty_file)\n",
    "\n",
    "# loading questions into different dataframes based on their quality\n",
    "eff_qbin_df = question_difficulty_df[['QID', 'question_quality']][question_difficulty_df['question_quality'] == 'high_quality']\n",
    "sweff_qbin_df = question_difficulty_df[['QID', 'question_quality']][question_difficulty_df['question_quality'] == 'average_quality']\n",
    "ineff_qbin_df = question_difficulty_df[['QID', 'question_quality']][question_difficulty_df['question_quality'] == 'low_quality']\n",
    "\n",
    "\n",
    "\n",
    "# divide ratings by performance\n",
    "ratings_and_high_pf_df, ratings_and_average_pf_df, ratings_and_low_pf_df = by_performance_ratings(performance_df, quality_ratings_df)\n",
    "\n",
    "# creating a data frame containing ratings given by each group of students to each question  \n",
    "students_quality_ratings_df = merge_all_student_ratings(ratings_and_high_pf_df, ratings_and_average_pf_df, ratings_and_low_pf_df, quality_ratings_df, question_difficulty_df)\n",
    "\n",
    "#merging student rating and domain expert ratings\n",
    "full_quality_ratings_df = create_complete_dataframe(students_quality_ratings_df, expert_ratings_df)\n",
    "\n",
    "\n",
    "# ratings given by each group of students to each bin of questions\n",
    "effective_question_ratings = by_bin_ratings(eff_qbin_df, full_quality_ratings_df)\n",
    "effective_question_ratings[['Low-mean', 'Low-std', 'High-mean', 'High-std', 'Average-mean', 'Average-std', 'Expert Ratings' , 'Expert -std', 'Difficulty']].agg([np.mean])\n",
    "\n",
    "somewhat_effective_question_ratings = by_bin_ratings(sweff_qbin_df, full_quality_ratings_df)\n",
    "somewhat_effective_question_ratings[['Low-mean', 'Low-std', 'High-mean', 'High-std', 'Average-mean', 'Average-std', 'Expert Ratings' , 'Expert -std', 'Difficulty']].agg([np.mean])\n",
    "\n",
    "ineffective_question_ratings = by_bin_ratings(ineff_qbin_df, full_quality_ratings_df)\n",
    "ineffective_question_ratings[['Low-mean', 'Low-std', 'High-mean', 'High-std', 'Average-mean', 'Average-std', 'Expert Ratings' , 'Expert -std', 'Difficulty']].agg([np.mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Data Peparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Driven Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Rating Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis is removed for the sake of data privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student performance (mean and SD in each group:high, average, low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This analysis is commentet for the sake of data privacy\n",
    "\n",
    "# [high_performing_mark_mean, high_performing_mark_std] = high_performing_df['Mark'].agg([np.mean, np.std])\n",
    "# [av_performing_mark_mean, av_performing_mark_std] = average_performing_df['Mark'].agg([np.mean, np.std])\n",
    "# [low_performing_mark_mean, low_performing_mark_std] = low_performing_df['Mark'].agg([np.mean, np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prvar(high_performing_mark_mean)\n",
    "# prvar(high_performing_mark_std)\n",
    "# prvar(av_performing_mark_mean)\n",
    "# prvar(av_performing_mark_std)\n",
    "# prvar(low_performing_mark_mean)\n",
    "# prvar(low_performing_mark_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who is more willing to rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total and average activity (answering question) by each group of students on all questions on the platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [act_high_tot, act_high_mean, act_high_std] = high_performing_df['ActivityCount'].agg([np.sum, np.mean, np.std])\n",
    "# [act_av_tot, act_av_mean, act_av_std] = average_performing_df['ActivityCount'].agg([np.sum, np.mean, np.std])\n",
    "# [act_low_tot, act_low_mean, act_low_std] = low_performing_df['ActivityCount'].agg([np.sum, np.mean, np.std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prvar(act_high_tot)\n",
    "# prvar(act_high_mean)\n",
    "# prvar(act_av_tot)\n",
    "# prvar(act_av_mean)\n",
    "# prvar(act_low_tot)\n",
    "# prvar(act_low_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total and average ratings by each group of students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_performing_ratings = pd.merge(high_performing_df, Question_ratings, on = ['UserID'])\n",
    "# av_performing_ratings = pd.merge(average_performing_df, Question_ratings, on = ['UserID'])\n",
    "# l_performing_ratings = pd.merge(low_performing_df, Question_ratings, on = ['UserID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating_high_tot = len(h_performing_ratings)\n",
    "# rating_high_mean = len(h_performing_ratings)/high_performing_df['UserID'].nunique()\n",
    "\n",
    "# rating_av_tot = len(av_performing_ratings)\n",
    "# rating_av_mean = len(av_performing_ratings)/average_performing_df['UserID'].nunique()\n",
    "\n",
    "# rating_low_tot = len(l_performing_ratings)\n",
    "# rating_low_mean = len(l_performing_ratings)/low_performing_df['UserID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prvar(rating_high_tot)\n",
    "# prvar(rating_high_mean)\n",
    "# prvar(rating_av_tot)\n",
    "# prvar(rating_av_mean)\n",
    "# prvar(rating_low_tot)\n",
    "# prvar(rating_low_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = av_performing_ratings['UserID'].value_counts()\n",
    "# prvar(np.mean(a.values))\n",
    "# prvar(np.std(a.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of ratings by each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot = full_effectiveness_ratings.boxplot(column=['Low-mean', 'High-mean', 'Average-mean', 'Expert Ratings'], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Error Analysis \n",
    "## +\n",
    "## Analysis 4: Time Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE (Static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_h = sqrt(mean_squared_error(full_quality_ratings_df['High-mean'], full_quality_ratings_df['Expert Ratings']))\n",
    "rmse_l = sqrt(mean_squared_error(full_quality_ratings_df['Low-mean'], full_quality_ratings_df['Expert Ratings']))\n",
    "rmse_av = sqrt(mean_squared_error(full_quality_ratings_df['Average-mean'], full_quality_ratings_df['Expert Ratings']))\n",
    "rmse_class = sqrt(mean_squared_error(full_quality_ratings_df['Class-mean'], full_quality_ratings_df['Expert Ratings'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prvar(rmse_h)\n",
    "prvar(rmse_l)\n",
    "prvar(rmse_av)\n",
    "prvar(rmse_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error over time (RMSE over time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high_performing students and experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekly_rmse_func(ratings_ot_df, expert_ratings):\n",
    "    #converting the create_at column of the dataframe to a meaningful datetime type for python\n",
    "    ratings_df = ratings_ot_df.copy()\n",
    "    ratings_df['date'] = ratings_df['created_at']\n",
    "#     ratings_df['date'] = pd.to_datetime(ratings_df.date, format='%d/%m/%Y') \n",
    "    ratings_df['date'] = pd.to_datetime(ratings_df['date']) \n",
    "    \n",
    "    #tag each interaction by the week it was happened\n",
    "    ratings_df['week_number_of_year'] = ratings_df['date'].dt.week\n",
    "    \n",
    "    #sorting values based on the week of the year \n",
    "    ratings_df = ratings_df.sort_values(['week_number_of_year'])\n",
    "    \n",
    "    # 1- getting the unique number of weeks in the dataset\n",
    "    weeks = ratings_df['week_number_of_year'].unique()\n",
    "    weeks.sort()\n",
    "    \n",
    "    prvar(weeks)\n",
    "    \n",
    "    weekly_RMSE = []\n",
    "    for week in weeks:\n",
    "        temp_df = pd.DataFrame(columns = ratings_df.columns)\n",
    "        temp_df = ratings_df.loc[(ratings_df.week_number_of_year == week)]\n",
    "    \n",
    "\n",
    "        # average question efectiveness in each week and convert the findings into a dataframe\n",
    "        temp_df = pd.merge(temp_df, expert_ratings, on=['QID'])\n",
    "        rmse = sqrt(mean_squared_error(temp_df['Expert Ratings'], temp_df['effectiveness']))\n",
    "        weekly_RMSE.append([week-29, rmse])\n",
    "    \n",
    "    # removing the last two weeks as they are inclluded in the 13 weeks duration of the semester\n",
    "    weekly_RMSE.remove(weekly_RMSE[-1])\n",
    "    weekly_RMSE.remove(weekly_RMSE[-1])\n",
    "    return weekly_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hperforming_weekly_RMSE = weekly_rmse_func(ratings_and_high_pf_df, expert_ratings_df)\n",
    "avperforming_weekly_RMSE = weekly_rmse_func(ratings_and_average_pf_df, expert_ratings_df)\n",
    "lperforming_weekly_RMSE = weekly_rmse_func(ratings_and_low_pf_df, expert_ratings_df)\n",
    "\n",
    "\n",
    "week, h_weekly_rmse = map(list, zip(*hperforming_weekly_RMSE))\n",
    "week, l_weekly_rmse = map(list, zip(*lperforming_weekly_RMSE))\n",
    "week, avg_weekly_rmse = map(list, zip(*avperforming_weekly_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(18,6))\n",
    "# # fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# plt.subplot(1,3,1)\n",
    "# x = np.arange(13) + 1\n",
    "# prvar(x)\n",
    "# y_h = h_weekly_rmse\n",
    "# prvar(y_h)\n",
    "# y_avg = avg_weekly_rmse\n",
    "# y_l = l_weekly_rmse\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(x,y_h)\n",
    "# prvar(p_value)\n",
    "# prvar(r_value)\n",
    "# line_h = slope*x+intercept\n",
    "# f1 = plt.scatter(list(x), list(y_h), c ='black', label='RMSE', s = 65)\n",
    "# l1 = plt.plot(x, line_h, color = 'black', label='Fitted line')\n",
    "# plt.xlabel('High performing', fontsize=14)\n",
    "# plt.ylabel('Weekly RMSE', fontsize=14)\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "# plt.subplot(132)\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(x,y_avg)\n",
    "# line_avg = slope*x+intercept\n",
    "\n",
    "# plt.scatter(list(x), list(y_avg), c ='black', label='RMSE', s = 65)\n",
    "# plt.plot(x, line_avg, color = 'black', label='Fitted line')\n",
    "# plt.xlabel('Average performing', fontsize=14)\n",
    "# plt.legend()\n",
    "# prvar(p_value)\n",
    "# prvar(r_value)\n",
    "\n",
    "\n",
    "# plt.subplot(133)\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(x,y_l)\n",
    "# line_l = slope*x+intercept\n",
    "\n",
    "# plt.scatter(list(x), list(y_l), label='RMSE',  c ='black' , s= 65)\n",
    "# plt.plot(x, line_l, color = 'black', label='Fitted line')\n",
    "# plt.xlabel('Low performing', fontsize=14)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# prvar(p_value)\n",
    "# prvar(r_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3: Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "plt.subplot(1,3,1)\n",
    "n = full_quality_ratings_df['QID'].values \n",
    "x_h = (full_quality_ratings_df['High-mean']).values\n",
    "y_h = (full_quality_ratings_df['Expert Ratings']).values\n",
    "z_h = full_quality_ratings_df['Difficulty'].values\n",
    "z_h = (1 - z_h) \n",
    "z_h = list(z_h)\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_h,y_h)\n",
    "line_h = slope*x_h+intercept\n",
    "prvar(p_value)\n",
    "prvar(r_value)\n",
    "t_h = z_h\n",
    "f1 = plt.scatter(list(x_h), list(y_h), label='Data', c =t_h, cmap = 'coolwarm', s = 65)\n",
    "plt.xticks(np.arange(2,5, 0.5))\n",
    "l1 = plt.plot(x_h, line_h, color = 'black', label='Fitted line')\n",
    "plt.xlabel('High performing', fontsize=14)\n",
    "plt.ylabel('Domain Experts', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(132)\n",
    "\n",
    "x_avg = (full_quality_ratings_df['Average-mean']).values\n",
    "y_avg = (full_quality_ratings_df['Expert Ratings']).values\n",
    "z_avg = full_quality_ratings_df['Difficulty'].values\n",
    "z_avg = (1 - z_avg) \n",
    "z_avg = list(z_avg)\n",
    "t_avg = z_avg\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_avg,y_avg)\n",
    "prvar(p_value)\n",
    "prvar(r_value)\n",
    "line_avg = slope*x_avg+intercept\n",
    "f1 = plt.scatter(list(x_avg), list(y_avg), label='Data', c =t_avg, cmap = 'coolwarm', s = 65)\n",
    "\n",
    "plt.xticks(np.arange(2,5, 0.5))\n",
    "plt.plot(x_avg, line_avg, color = 'black', label='Fitted line')\n",
    "plt.xlabel('Average performing', fontsize=14)\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(133)\n",
    "\n",
    "x_l = (full_quality_ratings_df['Low-mean']).values\n",
    "y_l = (full_quality_ratings_df['Expert Ratings']).values\n",
    "z_l = full_quality_ratings_df['Difficulty'].values\n",
    "z_l = (1 - z_l) \n",
    "z_l = list(z_l)\n",
    "t_l = z_l \n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_l,y_l)\n",
    "line_l = slope*x_l+intercept\n",
    "prvar(p_value)\n",
    "prvar(r_value)\n",
    "f1 = plt.scatter(list(x_l), list(y_l), label='Data', c =t_l, cmap = 'coolwarm', s = 65)\n",
    "\n",
    "plt.xticks(np.arange(2,5, 0.5))\n",
    "plt.plot(x_l, line_l, color = 'black', label='Fitted line')\n",
    "plt.xlabel('Low performing', fontsize=14)\n",
    "plt.legend()\n",
    "plt.colorbar(f1)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_quality_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "plt.subplot(1,4,1)\n",
    "x_h = (full_quality_ratings_df['Class-mean']).values\n",
    "y_h = (full_quality_ratings_df['Expert Ratings']).values\n",
    "z_h = full_quality_ratings_df['Difficulty'].values\n",
    "z_h = (1 - z_h) * 150\n",
    "z_h = list(z_h)\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_h,y_h)\n",
    "line_h = slope*x_h+intercept\n",
    "prvar(p_value)\n",
    "prvar(r_value)\n",
    "\n",
    "# f1 = plt.scatter(list(x_h), list(y_h), s = z_h, label='Data', c ='r')\n",
    "plt.scatter(list(x_h), list(y_h), label='Ratings',  c ='black')\n",
    "# plt.colorbar(f1)\n",
    "plt.xticks(np.arange(2,5, 0.5))\n",
    "l1 = plt.plot(x_h, line_h, color = 'black', label='Fitted line')\n",
    "plt.xlabel('Class', fontsize=18)\n",
    "plt.ylabel('Domain Experts', fontsize=18)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "plt.subplot(142)\n",
    "\n",
    "x_avg = (full_quality_ratings_df['High-mean']).values\n",
    "y_avg = (full_quality_ratings_df['Expert Ratings']).values\n",
    "z_avg = full_quality_ratings_df['Difficulty'].values\n",
    "z_avg = (1 - z_avg) * 150\n",
    "z_avg = list(z_avg)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_avg,y_avg)\n",
    "prvar(p_value)\n",
    "prvar(r_value)\n",
    "line_avg = slope*x_avg+intercept\n",
    "\n",
    "# plt.scatter(list(x_l), list(y_l), s = z_l, label='Data', c ='g')\n",
    "plt.scatter(list(x_avg), list(y_avg), c ='black', label='Ratings')\n",
    "plt.xticks(np.arange(2,5, 0.5))\n",
    "plt.plot(x_avg, line_avg, color = 'black', label='Fitted line')\n",
    "plt.xlabel('High-performing', fontsize=18)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.show()\n",
    "\n",
    "plt.subplot(143)\n",
    "\n",
    "x_avg = (full_quality_ratings_df['Average-mean']).values\n",
    "y_avg = (full_quality_ratings_df['Expert Ratings']).values\n",
    "z_avg = full_quality_ratings_df['Difficulty'].values\n",
    "z_avg = (1 - z_avg) * 150\n",
    "z_avg = list(z_avg)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_avg,y_avg)\n",
    "prvar(p_value)\n",
    "prvar(r_value)\n",
    "line_avg = slope*x_avg+intercept\n",
    "\n",
    "# plt.scatter(list(x_l), list(y_l), s = z_l, label='Data', c ='g')\n",
    "plt.scatter(list(x_avg), list(y_avg), c ='black', label='Ratings')\n",
    "plt.xticks(np.arange(2,5, 0.5))\n",
    "plt.plot(x_avg, line_avg, color = 'black', label='Fitted line')\n",
    "plt.xlabel('Average-performing', fontsize=18)\n",
    "# plt.legend(fontsize=12)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "plt.subplot(144)\n",
    "\n",
    "x_l = (full_quality_ratings_df['Low-mean']).values\n",
    "y_l = (full_quality_ratings_df['Expert Ratings']).values\n",
    "z_l = full_quality_ratings_df['Difficulty'].values\n",
    "z_l = (1 - z_l) * 150\n",
    "z_l = list(z_l)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x_l,y_l)\n",
    "line_l = slope*x_l+intercept\n",
    "prvar(p_value)\n",
    "prvar(r_value)\n",
    "# plt.scatter(list(x_l), list(y_l), s = z_l, label='Data', c ='g')\n",
    "plt.scatter(list(x_l), list(y_l), label='Ratings',  c ='black' )\n",
    "plt.xticks(np.arange(2,5, 0.5))\n",
    "plt.plot(x_l, line_l, color = 'black', label='Fitted line')\n",
    "plt.xlabel('Low-performing', fontsize=18)\n",
    "# plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "# plt.scatter(list(x_l), list(y_l), label='ratings', c ='b')\n",
    "\n",
    "# plt.plot(x_l, line_l, label='Fitted line', color = 'black', ls = '--')\n",
    "# plt.xlabel('Subject Matter Experts', fontsize=14)\n",
    "# plt.ylabel('Low-performing', fontsize=14)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Data-Driven Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geneal funbctions for creating a student by question rating matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "creates a dictionary in which user ids are map to numbers: uDict\n",
    "'''\n",
    "def mapUsersToNumbers(rating_df):\n",
    "    map = {}\n",
    "    current =0\n",
    "    for index, row in rating_df.iterrows():\n",
    "        user = row['UserID']\n",
    "        if map.has_key(user)==False:\n",
    "            map[user]=current\n",
    "            current = current+1\n",
    "    return map, current \n",
    "\n",
    "'''\n",
    "creates a dictionary in which question names is mapped as distinct question name to dictionary: qDict\n",
    "'''\n",
    "def mapQuestionsToNumbers(rating_df):\n",
    "    map = {}\n",
    "    current =0\n",
    "    for index, row in rating_df.iterrows():\n",
    "        question = row['QID']\n",
    "        if map.has_key(question)==False:\n",
    "            map[question]=current\n",
    "            current = current+1\n",
    "    return map, current\n",
    "\n",
    "\n",
    "'''\n",
    "creates QT matrice: rows: question id, columns:topic id -> QMAT is oprganized based on qDict and tDict\n",
    "'''\n",
    "def createTMatrix(qDict, qSize, uDict, uSize, rating_df):\n",
    "    QTMat = np.zeros((qSize, uSize))\n",
    "    for index, row in rating_df.iterrows(): \n",
    "        qid = row['QID']\n",
    "        uid = row['UserID']\n",
    "        rating = row['effectiveness']\n",
    "        if qDict.has_key(qid)==True and uDict.has_key(uid)==True:\n",
    "            QTMat[qDict[qid]][uDict[uid]] =rating\n",
    "    return QTMat \n",
    "\n",
    "\n",
    "def createResponse_matrix(qDict, qSize, ex_rating_df):\n",
    "    QRMat = np.zeros((qSize, 1))\n",
    "    for index, row in ex_rating_df.iterrows(): \n",
    "        qid = row['QID']\n",
    "        rating = row['Expert Ratings']\n",
    "        if qDict.has_key(qid)==True:\n",
    "            QRMat[qDict[qid]] = rating\n",
    "        \n",
    "    return QRMat\n",
    "\n",
    "\n",
    "\n",
    "# writes matrix R into PathName/FileName\n",
    "def WriteMatrixToTable(PathName, FileName, R, uDict, qDict, headings):\n",
    "    '''\n",
    "    Writes the matrix R into PathName/FileName\n",
    "    '''\n",
    "    if not os.path.exists(PathName):   \n",
    "        os.makedirs(PathName)\n",
    "        \n",
    "    FilePath = PathName + \"/\" + FileName\n",
    "    fr = open(FilePath, 'wb')\n",
    "    try:\n",
    "        writer = csv.writer(fr)\n",
    "        if headings:\n",
    "            writer.writerow(headings)\n",
    "        for i in xrange(len(R)):\n",
    "            for j in xrange(len(R[i])):\n",
    "                if R[i][j] <> 0:\n",
    "                    writer.writerow([i, j, R[i][j]])\n",
    "    finally:\n",
    "        fr.close() \n",
    "\n",
    "\n",
    "def load(pathName, fileName, uDict, uSize, add_info, qDict, qSize, delimiter ):\n",
    "    '''\n",
    "    loads data from CSV file into a matrix\n",
    "    '''\n",
    "    train_set = np.zeros((uSize+add_info, qSize)) # add_info is added to handle different scenarios in the experiments\n",
    "    qfile = pathName + \"/\" + fileName\n",
    "    csv_file = csv.reader(open(qfile, \"rU\"), delimiter=delimiter)\n",
    "    i =0;\n",
    "    for row in csv_file:\n",
    "        id1 = int(row[0])\n",
    "        id2  = int(row[1])\n",
    "        rating = row[2]\n",
    "        train_set[id1][id2] = rating\n",
    "    return train_set\n",
    "\n",
    "\n",
    "def createTMatrix_reputation(qDict, qSize, rating_df):\n",
    "    '''\n",
    "    create a matrix based on the average of ratings \n",
    "    by each group of students and the question dictionary.\n",
    "    This matrix would be used by MF approaches that are \n",
    "    based on performance\n",
    "    '''\n",
    "    QTMat = np.zeros((qSize, 1))\n",
    "    for index, row in rating_df.iterrows(): \n",
    "        qid = row['QID']\n",
    "        h_rating = row['High-mean']\n",
    "        avg_rating = row['Average-mean']\n",
    "        l_rating = row['Low-mean']\n",
    "        if qDict.has_key(qid)==True:\n",
    "            QTMat[qDict[qid]][0] = h_rating\n",
    "#             QTMat[qDict[qid]][1] = avg_rating\n",
    "#             QTMat[qDict[qid]][2] = l_rating\n",
    "    return QTMat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uDict, uSize = mapUsersToNumbers(quality_ratings_df) # student dictionary\n",
    "qDict, qSize = mapQuestionsToNumbers(quality_ratings_df) # question dictionary\n",
    "QTMat = createTMatrix(qDict, qSize, uDict, uSize, quality_ratings_df)\n",
    "QRMat = createResponse_matrix(qDict, qSize, expert_ratings_df) # expert ratings based on the question dictionary\n",
    "\n",
    "QTMat_mean = np.true_divide(QTMat.sum(1),(QTMat!=0).sum(1)).reshape(-1, 1) # the average of ratings given by all students\n",
    "QTMat_reputation = createTMatrix_reputation(qDict, qSize, full_quality_ratings_df) # the average ratings given by each group of students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF with MyMediaLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfolder = 'recoms'\n",
    "recomfolder = 'recoms'\n",
    "trainFile = \"train.csv\"\n",
    "testFile = \"test.csv\"\n",
    "recommender='BiasedMatrixFactorization'\n",
    "predictionFile = 'predictions.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  RecSys(inputfolder, recomfolder, recommender, trainset, testset, output):\n",
    "    '''\n",
    "    recommender system with MyMediaLite\n",
    "    '''\n",
    "    executable = \"rating_prediction\"\n",
    "\n",
    "    check_output(\n",
    "                ['mono', executable,\n",
    "#                 '--recommender-options', \"num_iter=1000\",\n",
    "#                 '--recommender-options', \"num_factors=5\",\n",
    "                '--training-file', inputfolder + '/' + trainset,\n",
    "                '--test-file', inputfolder + '/'+ testset,\n",
    "                '--recommender', recommender,            \n",
    "                '--prediction-file' , recomfolder + '/' + output])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The two unsupervised MF-based approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "### Matrix factorization with K-fold Cross validation  ###\n",
    "##########################################################\n",
    "\n",
    "def spilt_data_cv(QTMat, QTMat_extra,  QRMat, uDict, qDict, indices, mode):\n",
    "    '''\n",
    "    mode: students -> only ratings from students and their mean or only ratings from g=high-performing students\n",
    "          students+performance -> ratings from students and their performance in 3 classes\n",
    "    '''\n",
    "    QTMat_T = QTMat.T\n",
    "    if mode == 'students':\n",
    "        QTMat_extra_T = QTMat_extra.T\n",
    "        QMAT = np.vstack((QTMat_T, QTMat_extra_T))\n",
    "        \n",
    "    elif mode == 'students+performance':\n",
    "        QTMat_extra_T = QTMat_extra.T\n",
    "        QMAT = np.vstack((QTMat_T, QTMat_extra_T))\n",
    "        \n",
    "        \n",
    "    train_set = QMAT.copy()\n",
    "    test_set = np.zeros_like(QMAT)\n",
    "    for index in indices:\n",
    "        if index != None:\n",
    "            if mode == 'students+performance':\n",
    "                train_set[-1, index] = 0\n",
    "                train_set[-2, index] = 0\n",
    "                train_set[-3, index] = 0\n",
    "                test_set[-1, index] = QMAT[-1, index]\n",
    "                test_set[-2, index] = QMAT[-2, index]\n",
    "                test_set[-3, index] = QMAT[-3, index]  \n",
    "            else:\n",
    "                train_set[-1, index] = 0\n",
    "                test_set[-1, index] = QMAT[-1, index]\n",
    "    WriteMatrixToTable(recomfolder, \"train.csv\", train_set, uDict, qDict,[])\n",
    "    WriteMatrixToTable(recomfolder, \"test.csv\", test_set, uDict, qDict,[])\n",
    "    return test_set\n",
    "\n",
    "\n",
    "def MF_cv(QTMat, QTMat_extra, QRMat, uDict, uSize, qDict, qSize, add_info, mode):\n",
    "    random.seed(0)\n",
    "    rmse_tot = []\n",
    "    indices = np.arange(42) # selecting 9 indices from expert row for the test set\n",
    "    indices\n",
    "    indices = np.append(indices, [None]*8)\n",
    "    random.shuffle(indices)\n",
    "    indices = np.reshape(indices, (-1, 5))\n",
    "    if mode == 'students+performance':\n",
    "        MF_predictions = np.zeros((qSize,3))\n",
    "        ground_truth = np.zeros((qSize, 1))\n",
    "        MF_predictions_list_h = [] \n",
    "        MF_predictions_list_avg = []\n",
    "        MF_predictions_list_l = []\n",
    "        ground_truth_list = []\n",
    "\n",
    "        for i in np.arange(len(indices)):\n",
    "            test = spilt_data_cv(QTMat, QTMat_extra, QRMat, uDict, qDict, indices[i], mode)\n",
    "            RecSys(recomfolder, recomfolder, recommender,trainFile, testFile, predictionFile)\n",
    "            Rprime = load(recomfolder, predictionFile, uDict, uSize, add_info, qDict, qSize, \"\t\")\n",
    "            for index in indices[i]:\n",
    "                if index != None:\n",
    "                    ground_truth[index] = QRMat[index]\n",
    "                    ground_truth_list.append(QRMat[index])\n",
    "                    MF_predictions[index, 0] = Rprime[-3, index]\n",
    "                    MF_predictions[index, 1] = Rprime[-2, index]\n",
    "                    MF_predictions[index, 2] = Rprime[-1, index]\n",
    "                    MF_predictions_list_h.append(Rprime[-3, index])\n",
    "                    MF_predictions_list_avg.append(Rprime[-2, index])\n",
    "                    MF_predictions_list_l.append(Rprime[-1, index])\n",
    "            rmse_h = sqrt(mean_squared_error(MF_predictions_list_h, ground_truth_list))\n",
    "            rmse_avg = sqrt(mean_squared_error(MF_predictions_list_avg, ground_truth_list))\n",
    "            rmse_l = sqrt(mean_squared_error(MF_predictions_list_l, ground_truth_list))\n",
    "            rmse_tot.append([rmse_h, rmse_avg, rmse_l])\n",
    "            MF_predictions_list_h = [] \n",
    "            MF_predictions_list_avg = []\n",
    "            MF_predictions_list_l = []\n",
    "            ground_truth_list = []\n",
    "\n",
    "        for i in np.arange(3):\n",
    "            rmse = sqrt(mean_squared_error(list(MF_predictions[:, i]), list(ground_truth)))\n",
    "        \n",
    "    else:\n",
    "        MF_predictions = np.zeros((qSize))\n",
    "        ground_truth = np.zeros((qSize))\n",
    "        MF_predictions_list = [] \n",
    "        ground_truth_list = []\n",
    "        for i in np.arange(len(indices)):\n",
    "            test = spilt_data_cv(QTMat, QTMat_extra, QRMat, uDict, qDict, indices[i], mode)\n",
    "            RecSys(recomfolder, recomfolder, recommender,trainFile, testFile, predictionFile)\n",
    "            Rprime = load(recomfolder, predictionFile, uDict, uSize, add_info, qDict, qSize, \"\t\")\n",
    "            for index in indices[i]:\n",
    "                if index != None:\n",
    "                    ground_truth[index] = QRMat[index]\n",
    "                    ground_truth_list.append(QRMat[index])\n",
    "                    MF_predictions_list.append(Rprime[-1, index])\n",
    "                    MF_predictions[index] = Rprime[-1, index]\n",
    "            rmse_tot.append(sqrt(mean_squared_error((MF_predictions_list), (ground_truth_list))))\n",
    "            MF_predictions_list = [] \n",
    "            ground_truth_list = []\n",
    "\n",
    "    return rmse_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_app1 = MF_cv(QTMat, QTMat_mean, QRMat, uDict, uSize, qDict, qSize, 1, 'students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse and std for the class based on the first MF-based approach\n",
    "rmse_app1_students = np.mean(rmse_app1)\n",
    "prvar(rmse_app1_students)\n",
    "\n",
    "std_app1_students = np.std(rmse_app1)\n",
    "prvar(std_app1_students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Students+Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the old version, mode should be 'student+performance'. This model predecits the rating from \n",
    "# each performance group of student point of view. The current version, predicts ratings from \n",
    "# high-perofrming students point of view\n",
    "rmse_app2 = MF_cv(QTMat, QTMat_reputation, QRMat, uDict, uSize, qDict, qSize,1, 'students')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_app2_students = np.mean(rmse_app2)\n",
    "prvar(rmse_app2_students)\n",
    "\n",
    "std_app2_students = np.std(rmse_app2)\n",
    "prvar(std_app2_students)\n",
    "\n",
    "# if mode is 'student+performance'\n",
    "# rmse_app2 = np.array(rmse_app2)\n",
    "\n",
    "# # rmse and std for high-performing\n",
    "# rmse_app2_hp = np.mean(rmse_app2[:, 0])\n",
    "# std_app2_hp = np.std(rmse_app2[:,0])\n",
    "# prvar(rmse_app2_hp)\n",
    "\n",
    "# #rmse and std for average-performing\n",
    "# rmse_app2_ap = np.mean(rmse_app2[:, 1])\n",
    "# std_app2_ap = np.std(rmse_app2[:,1])\n",
    "# prvar(rmse_app2_ap)\n",
    "\n",
    "# #rmse and std for low-performing\n",
    "# rmse_app2_lp = np.mean(rmse_app2[:, 2])\n",
    "# std_app2_lp = np.std(rmse_app2[:,2])\n",
    "# prvar(rmse_app2_lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The two semi-supervised MF-base approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spilt_data_cv_density(QTMat, QTMat_extra,  QRMat, uDict, qDict, indices, train_idx, mode):\n",
    "    '''\n",
    "    mode: students+experts -> ratings from students and experts ratings\n",
    "          students+performance+experts -> ratings from students, their performance and ratings from experts\n",
    "    ''' \n",
    "    QTMat_T = QTMat.T\n",
    "    QRMat_T = QRMat.T\n",
    "    QRMat_zeros_T = np.zeros_like(QRMat_T)\n",
    "    QRMat_zeros_T[0, train_idx] = QRMat_T[0, train_idx]\n",
    "    if mode == 'students+experts':\n",
    "        QMAT = np.vstack((QTMat_T,QRMat_zeros_T))\n",
    "    else:\n",
    "        QTMat_extra_T = QTMat_extra.T\n",
    "        QMAT = np.vstack((QTMat_T,QTMat_extra_T, QRMat_zeros_T))\n",
    "    \n",
    "    train_set = QMAT.copy()\n",
    "    test_set = np.zeros_like(QMAT)\n",
    "    for index in indices:\n",
    "        if index != None:\n",
    "            test_set[-1, index] = QRMat_T[0, index]\n",
    "    WriteMatrixToTable(recomfolder, \"train.csv\", train_set, uDict, qDict,[])\n",
    "    WriteMatrixToTable(recomfolder, \"test.csv\", test_set, uDict, qDict,[])\n",
    "    return test_set\n",
    "\n",
    "\n",
    "def MF_cv_density(QTMat, QTMat_extra, QRMat, uDict, uSize, qDict, qSize, add_info, mode):\n",
    "    random.seed(0)\n",
    "    rmse_tot = []\n",
    "    indices = np.arange(42) # selecting 9 indices from expert row for the test set\n",
    "    indices = np.append(indices, [None]*8) # adding 8 none values to the indices to make them dividable by 8\n",
    "    random.shuffle(indices)\n",
    "    indices = np.reshape(indices, (-1, 5)) # rearange indices into a 10 by 5 where values in each row determines indices to be in the test set\n",
    "        \n",
    "    MF_predictions = np.zeros((qSize))\n",
    "    ground_truth = np.zeros((qSize))\n",
    "    MF_predictions_list = [] \n",
    "    ground_truth_list = []\n",
    "    \n",
    "    for i in np.arange(len(indices)):\n",
    "        none_cntr = np.count_nonzero(indices[i] !=None) #counting the number of none values for that specific fold\n",
    "        test_fold_len = len(indices[i])\n",
    "        train_index = [x for x in np.arange(42) if x not in indices[i]] #indices that are not in the test fold and can be used in training\n",
    "        train_len = [2, 4, 6, 8]\n",
    "        for j in np.arange(len(train_len)):\n",
    "            train_indices = random.sample(train_index, train_len[j]) #j)\n",
    "            test = spilt_data_cv_density(QTMat, QTMat_extra, QRMat, uDict, qDict, indices[i], train_indices, mode)\n",
    "            RecSys(recomfolder, recomfolder, recommender,trainFile, testFile, predictionFile)\n",
    "            Rprime = load(recomfolder, predictionFile, uDict, uSize, add_info, qDict, qSize, \"\t\")\n",
    "            \n",
    "            for index in indices[i]:\n",
    "                if index != None:\n",
    "                    ground_truth[index] = QRMat[index]\n",
    "                    ground_truth_list.append(QRMat[index])\n",
    "                    MF_predictions_list.append(Rprime[-1, index])\n",
    "                    MF_predictions[index] = Rprime[-1, index]\n",
    "                    \n",
    "#                     prvar([j, sqrt(mean_squared_error((MF_predictions_list), (ground_truth_list)))])\n",
    "            if (j != (len(train_len)-1)):\n",
    "                rmse_tot.append([train_len[j], sqrt(mean_squared_error((MF_predictions_list), (ground_truth_list)))])\n",
    "            else:\n",
    "                rmse_tot.append([37, sqrt(mean_squared_error((MF_predictions_list), (ground_truth_list)))])\n",
    "                \n",
    "            MF_predictions_list = [] \n",
    "            ground_truth_list = []\n",
    "            \n",
    "    rmse_tot_df = pd.DataFrame(rmse_tot, columns=['density', 'RMSE'])\n",
    "    RMSE_mean = rmse_tot_df.groupby(['density'])['RMSE'].mean()\n",
    "    return RMSE_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student+Performance+Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_app3 = MF_cv_density(QTMat, QTMat_reputation,  QRMat, uDict, uSize, qDict, qSize,2, 'students+performance+experts')\n",
    "prvar(rmse_app3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student + Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_app4 = MF_cv_density(QTMat, None,  QRMat, uDict, uSize, qDict, qSize,1, 'students+experts')\n",
    "prvar(rmse_app4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
